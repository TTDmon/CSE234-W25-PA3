{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmRSlH1L5r-F"
      },
      "source": [
        "# CSE 234 Programming Assignment 3: Speculative Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDuj8yGG6EXg"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "52T8Gw-R5lup"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xunyijiang/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from typing import List, Tuple, Dict, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyZ4tAb6Gu2"
      },
      "source": [
        "## Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VIvmDG725x8H"
      },
      "outputs": [],
      "source": [
        "class SpeculativeDecoder:\n",
        "    def __init__(self, target_model_name: str, draft_model_name: str, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize the speculative decoder with target and draft models.\n",
        "\n",
        "        Args:\n",
        "            target_model_name: HuggingFace model ID for the larger target model.\n",
        "            draft_model_name: HuggingFace model ID for the smaller draft model.\n",
        "            device: Device to run models on (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.target_model, self.target_tokenizer = self.initialize_target_model(target_model_name)\n",
        "        self.draft_model, self.draft_tokenizer = self.initialize_draft_model(draft_model_name)\n",
        "\n",
        "        # Ensure tokenizers are compatible\n",
        "        assert self.target_tokenizer.vocab == self.draft_tokenizer.vocab, \"Tokenizers must be compatible\"\n",
        "\n",
        "    def initialize_target_model(self, model_name: str):\n",
        "        \"\"\"Initialize the larger target model with caching enabled and proper pad token.\"\"\"\n",
        "        print(f\"Loading target model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement target model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        # baseline: directly load the model\n",
        "        # pad token:\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        model.to(self.device)\n",
        "        return model, tokenizer\n",
        "\n",
        "    def initialize_draft_model(self, model_name: str):\n",
        "        \"\"\"\n",
        "        Initialize a smaller, faster draft model with proper pad token.\n",
        "        Uses lower precision and additional optimizations.\n",
        "        \"\"\"\n",
        "        print(f\"Loading draft model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement draft model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        if not hasattr(self, 'target_model'):\n",
        "            self.target_model, self.target_tokenizer = self.initialize_target_model(self.target_model_name)\n",
        "\n",
        "        # pad token:\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        # baseline: directly load the model \n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        model.to(self.device)\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def generate_draft_tokens(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
        "                             num_speculative_tokens: int = 10) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate speculative tokens in one forward call using the draft model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs (tensor of shape [1, seq_len]).\n",
        "            attention_mask: Corresponding attention mask.\n",
        "            num_speculative_tokens: Number of tokens to speculate.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [1, num_speculative_tokens] containing the draft tokens.\n",
        "        \"\"\"\n",
        "        # TODO: Implement draft token generation\n",
        "        # 1. Use the draft model to generate tokens\n",
        "        # 2. Extract only the new tokens (not including the input)\n",
        "        # 3. Return the newly generated tokens\n",
        "        # baseline: directly generate tokens\n",
        "        if not hasattr(self, 'draft_model'):\n",
        "            self.draft_model, self.draft_tokenizer = self.initialize_draft_model(self.draft_model_name)\n",
        "        \n",
        "        max_tokens = num_speculative_tokens + input_ids.shape[-1]\n",
        "        generation_config = GenerationConfig(\n",
        "            max_new_tokens=num_speculative_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.3,\n",
        "            pad_token_id=self.draft_tokenizer.pad_token_id,\n",
        "            eos_token_id=self.draft_tokenizer.eos_token_id,\n",
        "            bos_token_id=self.draft_tokenizer.bos_token_id,\n",
        "        )\n",
        "        self.draft_model.generation_config = generation_config\n",
        "        draft = self.draft_model.generate(\n",
        "            input_ids=input_ids, \n",
        "            attention_mask=attention_mask, \n",
        "            # use_cache=True\n",
        "        )\n",
        "\n",
        "        return draft[:, input_ids.shape[-1]:]\n",
        "\n",
        "    def verify_tokens_vectorized(self, input_ids: torch.Tensor, draft_tokens: torch.Tensor,\n",
        "                               attention_mask: torch.Tensor) -> Tuple[List[int], int]:\n",
        "        \"\"\"\n",
        "        Vectorized verification: verify all draft tokens in one forward pass using the target model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: The current input token IDs (shape [1, L]).\n",
        "            draft_tokens: Draft tokens from the draft model (shape [1, k]).\n",
        "            attention_mask: The current attention mask for input_ids.\n",
        "\n",
        "        Returns:\n",
        "            accepted_tokens: List of accepted token IDs.\n",
        "            accepted_position: Index of the first rejected token (if all accepted, equals draft_tokens.shape[1]).\n",
        "        \"\"\"\n",
        "        # TODO: Implement efficient verification of draft tokens\n",
        "        # 1. Run target model on input_ids concatenated with draft_tokens\n",
        "        # 2. Extract the logits for positions where draft tokens would be predicted\n",
        "        # 3. Compare target model predictions with draft tokens\n",
        "        # 4. Determine how many consecutive tokens were accepted before first mismatch\n",
        "        target_input_ids = torch.cat([input_ids, draft_tokens], dim=-1)\n",
        "        target_attention_mask = torch.cat([attention_mask, torch.ones_like(draft_tokens)], dim=-1)\n",
        "        target_input_ids = target_input_ids.to(self.device)\n",
        "        target_attention_mask = target_attention_mask.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.target_model(target_input_ids, attention_mask=target_attention_mask).logits\n",
        "        \n",
        "        draft_start_idx = input_ids.shape[-1]-1\n",
        "        target_draft_logits = logits[:, draft_start_idx:-1, :]\n",
        "\n",
        "        # get target tokens\n",
        "        target_predicted_tokens = target_draft_logits.argmax(dim=-1)\n",
        "\n",
        "        # matches tokens\n",
        "        match_mask = ~(target_predicted_tokens == draft_tokens).int()\n",
        "        if match_mask.sum() != 0:\n",
        "            accept_position = torch.argmax(match_mask, dim=-1)\n",
        "        else:\n",
        "            accept_position = draft_tokens.shape[-1]\n",
        "\n",
        "        accepted_tokens = draft_tokens[:,:accept_position]\n",
        "        return accepted_tokens, accept_position\n",
        "\n",
        "    def speculative_decode(self, prompt: str, max_tokens: int = 100,\n",
        "                          num_speculative_tokens: int = 15) -> str:\n",
        "        \"\"\"\n",
        "        Main speculative decoding algorithm with vectorized verification.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate (excluding prompt).\n",
        "            num_speculative_tokens: Number of tokens to speculate per iteration.\n",
        "\n",
        "        Returns:\n",
        "            Generated text.\n",
        "        \"\"\"\n",
        "        # Tokenize prompt\n",
        "        inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "        prompt_length = input_ids.shape[1]\n",
        "\n",
        "        # Initialize counters for performance tracking\n",
        "        total_tokens_generated = prompt_length\n",
        "        total_draft_tokens_proposed = 0\n",
        "        total_draft_tokens_accepted = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # TODO: Implement the core speculative decoding loop\n",
        "        # 1. Generate draft tokens using the draft model\n",
        "        # 2. Verify draft tokens using the target model\n",
        "        # 3. Accept verified tokens and append to the sequence\n",
        "        # 4. For rejected tokens or if all tokens are accepted, generate a new token with the target model\n",
        "        # 5. Stop when max_tokens is reached or an EOS token is generated\n",
        "        \n",
        "        while total_tokens_generated - prompt_length < max_tokens and input_ids[0, -1] != self.target_tokenizer.eos_token_id:   \n",
        "            # draft generation:\n",
        "            draft_ids = self.generate_draft_tokens(input_ids, attention_mask, num_speculative_tokens)\n",
        "            total_tokens_generated += num_speculative_tokens\n",
        "            total_draft_tokens_proposed += num_speculative_tokens\n",
        "\n",
        "            # verification:\n",
        "            accepted_draft_ids, accepted_position = self.verify_tokens_vectorized(input_ids, draft_ids, attention_mask)\n",
        "            total_draft_tokens_accepted += accepted_draft_ids.shape[-1]\n",
        "\n",
        "            # append accepted tokens to the sequence\n",
        "            input_ids = torch.cat([input_ids, accepted_draft_ids], dim=-1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones_like(accepted_draft_ids)], dim=-1)\n",
        "\n",
        "            # update counters\n",
        "            total_tokens_generated = input_ids.shape[-1]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        elapsed_time = time.time() - start_time\n",
        "        acceptance_rate = total_draft_tokens_accepted / total_draft_tokens_proposed if total_draft_tokens_proposed > 0 else 0\n",
        "\n",
        "        print(f\"Generated {total_tokens_generated - prompt_length} tokens in {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Tokens per second: {(total_tokens_generated - prompt_length) / elapsed_time:.2f}\")\n",
        "        print(f\"Draft token acceptance rate: {acceptance_rate:.2%}\")\n",
        "\n",
        "        return self.target_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def benchmark(self, prompt: str, max_tokens: int = 100,\n",
        "                  num_runs: int = 3, compare_baseline: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Benchmark the speculative decoder against baseline decoding.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate.\n",
        "            num_runs: Number of benchmark runs.\n",
        "            compare_baseline: Whether to compare with baseline (non-speculative) decoding.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with benchmark results.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"speculative\": {\"times\": [], \"tokens_per_second\": []},\n",
        "            \"baseline\": {\"times\": [], \"tokens_per_second\": []} if compare_baseline else None\n",
        "        }\n",
        "\n",
        "        # Benchmark speculative decoding.\n",
        "        for _ in range(num_runs):\n",
        "            start_time = time.time()\n",
        "            output = self.speculative_decode(prompt, max_tokens=max_tokens)\n",
        "            elapsed = time.time() - start_time\n",
        "            prompt_len = len(self.target_tokenizer(prompt)[\"input_ids\"])\n",
        "            output_tokens = len(self.target_tokenizer.encode(output)) - prompt_len\n",
        "            tps = output_tokens / elapsed\n",
        "            results[\"speculative\"][\"times\"].append(elapsed)\n",
        "            results[\"speculative\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        # Benchmark baseline decoding.\n",
        "        if compare_baseline:\n",
        "            for _ in range(num_runs):\n",
        "                inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "                input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "                attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "                with torch.no_grad():\n",
        "                    output_ids = self.target_model.generate(\n",
        "                        input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_length=input_ids.shape[1] + max_tokens,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=self.target_tokenizer.pad_token_id\n",
        "                    )\n",
        "                elapsed = time.time() - start_time\n",
        "                output_tokens = output_ids.shape[1] - input_ids.shape[1]\n",
        "                tps = output_tokens / elapsed\n",
        "                results[\"baseline\"][\"times\"].append(elapsed)\n",
        "                results[\"baseline\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        for method in results.keys():\n",
        "            if results[method] is not None:\n",
        "                avg_time = sum(results[method][\"times\"]) / num_runs\n",
        "                avg_tps = sum(results[method][\"tokens_per_second\"]) / num_runs\n",
        "                results[method][\"avg_time\"] = avg_time\n",
        "                results[method][\"avg_tokens_per_second\"] = avg_tps\n",
        "\n",
        "        if compare_baseline:\n",
        "            speedup = results[\"baseline\"][\"avg_time\"] / results[\"speculative\"][\"avg_time\"]\n",
        "            results[\"speedup\"] = speedup\n",
        "            results[\"latency_reduction\"] = (1 - results[\"speculative\"][\"avg_time\"] / results[\"baseline\"][\"avg_time\"]) * 100\n",
        "            # print(f\"Speculative decoding speedup: {speedup:.2f}x\")\n",
        "            # print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNzh3cG-6KM0"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "YyNXbA-26Cpy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading draft model: EleutherAI/pythia-160m-deduped\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBenchmarking Prompt \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbenchmark\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_runs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompare_baseline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     29\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage speculative decoding time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeculative\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_time\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAverage speculative tokens per second: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspeculative\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mavg_tokens_per_second\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[0;32mIn[3], line 222\u001b[0m, in \u001b[0;36mSpeculativeDecoder.benchmark\u001b[0;34m(self, prompt, max_tokens, num_runs, compare_baseline)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_runs):\n\u001b[1;32m    221\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 222\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspeculative_decode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m     elapsed \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    224\u001b[0m     prompt_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_tokenizer(prompt)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
            "Cell \u001b[0;32mIn[3], line 175\u001b[0m, in \u001b[0;36mSpeculativeDecoder.speculative_decode\u001b[0;34m(self, prompt, max_tokens, num_speculative_tokens)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# TODO: Implement the core speculative decoding loop\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# 1. Generate draft tokens using the draft model\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# 2. Verify draft tokens using the target model\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;66;03m# 3. Accept verified tokens and append to the sequence\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# 4. For rejected tokens or if all tokens are accepted, generate a new token with the target model\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# 5. Stop when max_tokens is reached or an EOS token is generated\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m total_tokens_generated \u001b[38;5;241m-\u001b[39m prompt_length \u001b[38;5;241m<\u001b[39m max_tokens \u001b[38;5;129;01mand\u001b[39;00m input_ids[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_tokenizer\u001b[38;5;241m.\u001b[39meos_token_id:   \n\u001b[1;32m    174\u001b[0m     \u001b[38;5;66;03m# draft generation:\u001b[39;00m\n\u001b[0;32m--> 175\u001b[0m     draft_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_draft_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_speculative_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m     total_tokens_generated \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_speculative_tokens\n\u001b[1;32m    177\u001b[0m     total_draft_tokens_proposed \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m num_speculative_tokens\n",
            "Cell \u001b[0;32mIn[3], line 91\u001b[0m, in \u001b[0;36mSpeculativeDecoder.generate_draft_tokens\u001b[0;34m(self, input_ids, attention_mask, num_speculative_tokens)\u001b[0m\n\u001b[1;32m     82\u001b[0m generation_config \u001b[38;5;241m=\u001b[39m GenerationConfig(\n\u001b[1;32m     83\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39mnum_speculative_tokens,\n\u001b[1;32m     84\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m     bos_token_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraft_tokenizer\u001b[38;5;241m.\u001b[39mbos_token_id,\n\u001b[1;32m     89\u001b[0m )\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdraft_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m generation_config\n\u001b[0;32m---> 91\u001b[0m draft \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdraft_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     93\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# use_cache=True\u001b[39;49;00m\n\u001b[1;32m     95\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m draft[:, input_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:]\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/utils.py:2255\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2247\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2248\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2249\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2250\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2251\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2252\u001b[0m     )\n\u001b[1;32m   2254\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2255\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2256\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2265\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2266\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2267\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2268\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2269\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2274\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2275\u001b[0m     )\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/generation/utils.py:3257\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3255\u001b[0m     is_prefill \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   3259\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[1;32m   3260\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[1;32m   3261\u001b[0m     outputs,\n\u001b[1;32m   3262\u001b[0m     model_kwargs,\n\u001b[1;32m   3263\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   3264\u001b[0m )\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:1115\u001b[0m, in \u001b[0;36mGPTNeoXForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, inputs_embeds, head_mask, past_key_values, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position, **kwargs)\u001b[0m\n\u001b[1;32m   1086\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1087\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;124;03m    Labels for computing the left-to-right language modeling loss (next word prediction). Indices should be in\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;124;03m>>> prediction_logits = outputs.logits\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;124;03m```\"\"\"\u001b[39;00m\n\u001b[1;32m   1113\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1115\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpt_neox\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1129\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1130\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_out(hidden_states)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:887\u001b[0m, in \u001b[0;36mGPTNeoXModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    874\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    875\u001b[0m         layer\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    876\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    884\u001b[0m         position_embeddings,\n\u001b[1;32m    885\u001b[0m     )\n\u001b[1;32m    886\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 887\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    891\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    893\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    894\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    895\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    896\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    898\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    899\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:634\u001b[0m, in \u001b[0;36mGPTNeoXLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, use_cache, layer_past, output_attentions, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    624\u001b[0m     hidden_states: Optional[torch\u001b[38;5;241m.\u001b[39mFloatTensor],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    632\u001b[0m     position_embeddings: Optional[Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,  \u001b[38;5;66;03m# necessary, but kept here for BC\u001b[39;00m\n\u001b[1;32m    633\u001b[0m ):\n\u001b[0;32m--> 634\u001b[0m     attention_layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    637\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    638\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    639\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    640\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    643\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    644\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    645\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m attention_layer_outputs[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# output_attn: attn_output, present, (attn_weights)\u001b[39;00m\n\u001b[1;32m    646\u001b[0m     attn_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpost_attention_dropout(attn_output)\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:321\u001b[0m, in \u001b[0;36mGPTNeoXAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, head_mask, layer_past, use_cache, output_attentions, padding_mask, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    318\u001b[0m bsz, seq_len, _ \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Apply attention-specific projections and rope\u001b[39;00m\n\u001b[0;32m--> 321\u001b[0m query, key, value, present \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_attn_projections_and_rope\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# Checking for fallbacks in case an unsupported feature is requested\u001b[39;00m\n\u001b[1;32m    331\u001b[0m attention_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/models/gpt_neox/modeling_gpt_neox.py:446\u001b[0m, in \u001b[0;36mGPTNeoXAttention._attn_projections_and_rope\u001b[0;34m(self, hidden_states, position_ids, layer_past, use_cache, cache_position, position_embeddings)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m layer_past \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m     cache_kwargs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msin\u001b[39m\u001b[38;5;124m\"\u001b[39m: sin,\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcos\u001b[39m\u001b[38;5;124m\"\u001b[39m: cos,\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_rotation_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_ndims,\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcache_position\u001b[39m\u001b[38;5;124m\"\u001b[39m: cache_position,\n\u001b[1;32m    445\u001b[0m     }\n\u001b[0;32m--> 446\u001b[0m     key, value \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_past\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m query, key, value, layer_past\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/cache_utils.py:407\u001b[0m, in \u001b[0;36mDynamicCache.update\u001b[0;34m(self, key_states, value_states, layer_idx, cache_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    402\u001b[0m \u001b[38;5;124;03m    Support for backwards-compatible `past_key_value` length, e.g. `len(past_key_value)`. This value corresponds\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;124;03m    to the number of layers in the model.\u001b[39;00m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkey_cache)\n\u001b[0;32m--> 407\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mupdate\u001b[39m(\n\u001b[1;32m    408\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    409\u001b[0m     key_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    410\u001b[0m     value_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m    411\u001b[0m     layer_idx: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m    412\u001b[0m     cache_kwargs: Optional[Dict[\u001b[38;5;28mstr\u001b[39m, Any]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    413\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    414\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;124;03m    Updates the cache with the new `key_states` and `value_states` for the layer `layer_idx`.\u001b[39;00m\n\u001b[1;32m    416\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;124;03m        A tuple containing the updated key and value states.\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# Update the number of seen tokens\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading draft model: EleutherAI/pythia-160m-deduped\n"
          ]
        }
      ],
      "source": [
        "# # test function\n",
        "# target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "# draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# # Initialize speculative decoder\n",
        "# decoder = SpeculativeDecoder(\n",
        "#     target_model_name=target_model_name,\n",
        "#     draft_model_name=draft_model_name,\n",
        "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# )\n",
        "\n",
        "# # Test prompts\n",
        "# test_prompts = [\n",
        "#     \"The future of Artificial Intelligence is\",\n",
        "#     \"Write a short story about a robot learning to feel emotions:\",\n",
        "#     \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "# ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 0 tokens in 0.19 seconds\n",
            "Tokens per second: 0.00\n",
            "Draft token acceptance rate: 0.00%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The future of Artificial Intelligence is'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# decoder.speculative_decode(test_prompts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def generate_draft_tokens(model, draft_tokenizer,input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
        "#                             num_speculative_tokens: int = 10) -> torch.Tensor:\n",
        "#     \"\"\"\n",
        "#     Generate speculative tokens in one forward call using the draft model.\n",
        "\n",
        "#     Args:\n",
        "#         input_ids: Input token IDs (tensor of shape [1, seq_len]).\n",
        "#         attention_mask: Corresponding attention mask.\n",
        "#         num_speculative_tokens: Number of tokens to speculate.\n",
        "\n",
        "#     Returns:\n",
        "#         Tensor of shape [1, num_speculative_tokens] containing the draft tokens.\n",
        "#     \"\"\"\n",
        "#     # TODO: Implement draft token generation\n",
        "#     # 1. Use the draft model to generate tokens\n",
        "#     # 2. Extract only the new tokens (not including the input)\n",
        "#     # 3. Return the newly generated tokens\n",
        "#     # baseline: directly generate tokens\n",
        "\n",
        "    \n",
        "#     max_tokens = num_speculative_tokens + input_ids.shape[-1]\n",
        "#     generation_config = GenerationConfig(\n",
        "#         max_new_tokens=num_speculative_tokens,\n",
        "#         do_sample=True,\n",
        "#         pad_token_id=draft_tokenizer.pad_token_id,\n",
        "#         eos_token_id=draft_tokenizer.eos_token_id,\n",
        "#         bos_token_id=draft_tokenizer.bos_token_id,\n",
        "#     )\n",
        "#     model.generation_config = generation_config\n",
        "#     draft = model.generate(\n",
        "#         input_ids=input_ids, \n",
        "#         attention_mask=attention_mask, \n",
        "#         # use_cache=True\n",
        "#     )\n",
        "\n",
        "#     return draft[:, input_ids.shape[-1]:]\n",
        "\n",
        "# inputs = decoder.target_tokenizer(test_prompts[0], return_tensors=\"pt\", padding=True)\n",
        "# input_ids = inputs[\"input_ids\"].to(decoder.device)\n",
        "# attention_mask = inputs[\"attention_mask\"].to(decoder.device)\n",
        "# prompt_length = input_ids.shape[1]\n",
        "\n",
        "# # Initialize counters for performance tracking\n",
        "# total_tokens_generated = prompt_length\n",
        "# total_draft_tokens_proposed = 0\n",
        "# total_draft_tokens_accepted = 0\n",
        "# start_time = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 3])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 3])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 6])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 6])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 1])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 1])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 6])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 5])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 9])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "# total_tokens = 0\n",
        "# while total_tokens < 100 and input_ids[0, -1] != decoder.target_tokenizer.eos_token_id:\n",
        "#     draft_ids = generate_draft_tokens(decoder.draft_model, decoder.draft_tokenizer, input_ids, attention_mask, 10)\n",
        "\n",
        "#     # verification:\n",
        "#     target_input_ids = torch.cat([input_ids, draft_ids], dim=-1)\n",
        "#     target_attention_mask = torch.cat([attention_mask, torch.ones_like(draft_ids)], dim=-1)\n",
        "#     target_input_ids = target_input_ids.to(decoder.device)\n",
        "#     target_attention_mask = target_attention_mask.to(decoder.device)\n",
        "#     with torch.no_grad():\n",
        "#         logits = decoder.target_model(target_input_ids, attention_mask=target_attention_mask).logits\n",
        "        \n",
        "#     start_idx = input_ids.shape[-1]-1\n",
        "#     target_draft_logits = logits[:, start_idx:-1, :]\n",
        "\n",
        "#     # get target tokens\n",
        "#     target_predicted_tokens = target_draft_logits.argmax(dim=-1)\n",
        "\n",
        "#     # matches tokens\n",
        "#     match_mask = ~(target_predicted_tokens == draft_ids).int()\n",
        "#     if match_mask.sum() != 0:\n",
        "#         accept_position = torch.argmax(match_mask, dim=-1)\n",
        "#     else:\n",
        "#         accept_position = draft_ids.shape[-1]\n",
        "\n",
        "#     accepted_tokens = draft_ids[:,:accept_position]\n",
        "\n",
        "#     # append accepted tokens to the sequence\n",
        "#     print(accepted_tokens.shape)\n",
        "\n",
        "#     input_ids = torch.cat([input_ids, accepted_tokens], dim=-1)\n",
        "#     attention_mask = torch.cat([attention_mask, torch.ones_like(accepted_tokens)], dim=-1)\n",
        "#     total_tokens = input_ids.shape[-1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O1EORd26MdC"
      },
      "source": [
        "## Bonus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1sEo2706O29"
      },
      "outputs": [],
      "source": [
        "target_model_name = ...  # Larger target model\n",
        "draft_model_name = ...   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

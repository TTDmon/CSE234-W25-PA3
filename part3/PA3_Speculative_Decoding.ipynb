{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmRSlH1L5r-F"
      },
      "source": [
        "# CSE 234 Programming Assignment 3: Speculative Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDuj8yGG6EXg"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "52T8Gw-R5lup"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xunyijiang/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from typing import List, Tuple, Dict, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyZ4tAb6Gu2"
      },
      "source": [
        "## Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VIvmDG725x8H"
      },
      "outputs": [],
      "source": [
        "class SpeculativeDecoder:\n",
        "    def __init__(self, target_model_name: str, draft_model_name: str, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize the speculative decoder with target and draft models.\n",
        "\n",
        "        Args:\n",
        "            target_model_name: HuggingFace model ID for the larger target model.\n",
        "            draft_model_name: HuggingFace model ID for the smaller draft model.\n",
        "            device: Device to run models on (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.target_model, self.target_tokenizer = self.initialize_target_model(target_model_name)\n",
        "        self.draft_model, self.draft_tokenizer = self.initialize_draft_model(draft_model_name)\n",
        "\n",
        "        # Ensure tokenizers are compatible\n",
        "        assert self.target_tokenizer.vocab == self.draft_tokenizer.vocab, \"Tokenizers must be compatible\"\n",
        "\n",
        "    def initialize_target_model(self, model_name: str):\n",
        "        \"\"\"Initialize the larger target model with caching enabled and proper pad token.\"\"\"\n",
        "        print(f\"Loading target model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement target model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        # baseline: directly load the model\n",
        "        # pad token:\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        model.to(self.device)\n",
        "        return model, tokenizer\n",
        "\n",
        "    def initialize_draft_model(self, model_name: str):\n",
        "        \"\"\"\n",
        "        Initialize a smaller, faster draft model with proper pad token.\n",
        "        Uses lower precision and additional optimizations.\n",
        "        \"\"\"\n",
        "        print(f\"Loading draft model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement draft model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        if not hasattr(self, 'target_model'):\n",
        "            self.target_model, self.target_tokenizer = self.initialize_target_model(self.target_model_name)\n",
        "\n",
        "        # pad token:\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        # baseline: directly load the model \n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        model.to(self.device)\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def generate_draft_tokens(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
        "                             num_speculative_tokens: int = 10) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate speculative tokens in one forward call using the draft model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs (tensor of shape [1, seq_len]).\n",
        "            attention_mask: Corresponding attention mask.\n",
        "            num_speculative_tokens: Number of tokens to speculate.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [1, num_speculative_tokens] containing the draft tokens.\n",
        "        \"\"\"\n",
        "        # TODO: Implement draft token generation\n",
        "        # 1. Use the draft model to generate tokens\n",
        "        # 2. Extract only the new tokens (not including the input)\n",
        "        # 3. Return the newly generated tokens\n",
        "        # baseline: directly generate tokens\n",
        "        if not hasattr(self, 'draft_model'):\n",
        "            self.draft_model, self.draft_tokenizer = self.initialize_draft_model(self.draft_model_name)\n",
        "        \n",
        "        max_tokens = num_speculative_tokens + input_ids.shape[-1]\n",
        "        generation_config = GenerationConfig(\n",
        "            max_new_tokens=num_speculative_tokens,\n",
        "            do_sample=True,\n",
        "            # temperature=0.3,\n",
        "            pad_token_id=self.draft_tokenizer.pad_token_id,\n",
        "            eos_token_id=self.draft_tokenizer.eos_token_id,\n",
        "            bos_token_id=self.draft_tokenizer.bos_token_id,\n",
        "        )\n",
        "        self.draft_model.generation_config = generation_config\n",
        "        draft = self.draft_model.generate(\n",
        "            input_ids=input_ids, \n",
        "            attention_mask=attention_mask, \n",
        "            # use_cache=True\n",
        "        )\n",
        "\n",
        "        return draft[:, input_ids.shape[-1]:]\n",
        "\n",
        "    def verify_tokens_vectorized(self, input_ids: torch.Tensor, draft_tokens: torch.Tensor,\n",
        "                               attention_mask: torch.Tensor) -> Tuple[List[int], int]:\n",
        "        \"\"\"\n",
        "        Vectorized verification: verify all draft tokens in one forward pass using the target model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: The current input token IDs (shape [1, L]).\n",
        "            draft_tokens: Draft tokens from the draft model (shape [1, k]).\n",
        "            attention_mask: The current attention mask for input_ids.\n",
        "\n",
        "        Returns:\n",
        "            accepted_tokens: List of accepted token IDs.\n",
        "            accepted_position: Index of the first rejected token (if all accepted, equals draft_tokens.shape[1]).\n",
        "        \"\"\"\n",
        "        # TODO: Implement efficient verification of draft tokens\n",
        "        # 1. Run target model on input_ids concatenated with draft_tokens\n",
        "        # 2. Extract the logits for positions where draft tokens would be predicted\n",
        "        # 3. Compare target model predictions with draft tokens\n",
        "        # 4. Determine how many consecutive tokens were accepted before first mismatch\n",
        "        target_input_ids = torch.cat([input_ids, draft_tokens], dim=-1)\n",
        "        target_attention_mask = torch.cat([attention_mask, torch.ones_like(draft_tokens)], dim=-1)\n",
        "        target_input_ids = target_input_ids.to(self.device)\n",
        "        target_attention_mask = target_attention_mask.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.target_model(target_input_ids, attention_mask=target_attention_mask).logits\n",
        "        \n",
        "        draft_start_idx = input_ids.shape[-1]-1\n",
        "        target_draft_logits = logits[:, draft_start_idx:-1, :]\n",
        "\n",
        "        # get target tokens\n",
        "        target_predicted_tokens = target_draft_logits.argmax(dim=-1)\n",
        "\n",
        "        # matches tokens\n",
        "        match_mask = (target_predicted_tokens != draft_tokens).int() # 0 if match, 1 if mismatch\n",
        "\n",
        "        if match_mask.sum() != 0:\n",
        "            accept_position = torch.argmax(match_mask, dim=-1)\n",
        "        else:\n",
        "            accept_position = draft_tokens.shape[-1]\n",
        "\n",
        "        accepted_tokens = draft_tokens[:,:accept_position]\n",
        "        return accepted_tokens, accept_position\n",
        "\n",
        "    def speculative_decode(self, prompt: str, max_tokens: int = 100,\n",
        "                          num_speculative_tokens: int = 15) -> str:\n",
        "        \"\"\"\n",
        "        Main speculative decoding algorithm with vectorized verification.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate (excluding prompt).\n",
        "            num_speculative_tokens: Number of tokens to speculate per iteration.\n",
        "\n",
        "        Returns:\n",
        "            Generated text.\n",
        "        \"\"\"\n",
        "        # Tokenize prompt\n",
        "        inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "        prompt_length = input_ids.shape[1]\n",
        "\n",
        "        # Initialize counters for performance tracking\n",
        "        total_tokens_generated = prompt_length\n",
        "        total_draft_tokens_proposed = 0\n",
        "        total_draft_tokens_accepted = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # TODO: Implement the core speculative decoding loop\n",
        "        # 1. Generate draft tokens using the draft model\n",
        "        # 2. Verify draft tokens using the target model\n",
        "        # 3. Accept verified tokens and append to the sequence\n",
        "        # 4. For rejected tokens or if all tokens are accepted, generate a new token with the target model\n",
        "        # 5. Stop when max_tokens is reached or an EOS token is generated\n",
        "        \n",
        "        while total_tokens_generated - prompt_length < max_tokens and input_ids[0, -1] != self.target_tokenizer.eos_token_id:   \n",
        "            # draft generation:\n",
        "            draft_ids = self.generate_draft_tokens(input_ids, attention_mask, num_speculative_tokens)\n",
        "            total_draft_tokens_proposed += num_speculative_tokens\n",
        "\n",
        "            # verification:\n",
        "            accepted_draft_ids, accepted_position = self.verify_tokens_vectorized(input_ids, draft_ids, attention_mask)\n",
        "            total_draft_tokens_accepted += accepted_draft_ids.shape[-1]\n",
        "            # print(\"accepted_draft_ids\", accepted_draft_ids)\n",
        "\n",
        "            # print(\"before concatenating accepted_draft_ids:\", input_ids.shape[-1])\n",
        "\n",
        "            # append accepted tokens to the sequence\n",
        "            input_ids = torch.cat([input_ids, accepted_draft_ids], dim=-1)\n",
        "            attention_mask = torch.cat([attention_mask, torch.ones_like(accepted_draft_ids)], dim=-1)\n",
        "            \n",
        "            # print(\"Before concatenating new token:\", input_ids.shape[-1])\n",
        "            # if all accepted or rejected, generate a new token with the target model\n",
        "            if accepted_position == num_speculative_tokens or accepted_position == 0:\n",
        "                # print(\"Start generating a new token!!!!\")\n",
        "                # all accepted, generate a new token with the target model\n",
        "                input_ids = self.target_model.generate(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    max_new_tokens=1,\n",
        "                    do_sample=False,\n",
        "                    pad_token_id=self.target_tokenizer.pad_token_id\n",
        "                )\n",
        "                # print(\"new_token\", input_ids)\n",
        "                attention_mask = torch.cat([attention_mask, torch.tensor([[1]]).to(self.device)], dim=-1)\n",
        "\n",
        "            # update counters\n",
        "            total_tokens_generated = input_ids.shape[-1]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        elapsed_time = time.time() - start_time\n",
        "        acceptance_rate = total_draft_tokens_accepted / total_draft_tokens_proposed if total_draft_tokens_proposed > 0 else 0\n",
        "\n",
        "        print(f\"Generated {total_tokens_generated - prompt_length} tokens in {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Tokens per second: {(total_tokens_generated - prompt_length) / elapsed_time:.2f}\")\n",
        "        print(f\"Draft token acceptance rate: {acceptance_rate:.2%}\")\n",
        "\n",
        "        return self.target_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def benchmark(self, prompt: str, max_tokens: int = 100,\n",
        "                  num_runs: int = 3, compare_baseline: bool = True) -> Dict:\n",
        "        \"\"\"\n",
        "        Benchmark the speculative decoder against baseline decoding.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate.\n",
        "            num_runs: Number of benchmark runs.\n",
        "            compare_baseline: Whether to compare with baseline (non-speculative) decoding.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with benchmark results.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"speculative\": {\"times\": [], \"tokens_per_second\": []},\n",
        "            \"baseline\": {\"times\": [], \"tokens_per_second\": []} if compare_baseline else None\n",
        "        }\n",
        "\n",
        "        # Benchmark speculative decoding.\n",
        "        for _ in range(num_runs):\n",
        "            start_time = time.time()\n",
        "            output = self.speculative_decode(prompt, max_tokens=max_tokens)\n",
        "            elapsed = time.time() - start_time\n",
        "            prompt_len = len(self.target_tokenizer(prompt)[\"input_ids\"])\n",
        "            output_tokens = len(self.target_tokenizer.encode(output)) - prompt_len\n",
        "            tps = output_tokens / elapsed\n",
        "            results[\"speculative\"][\"times\"].append(elapsed)\n",
        "            results[\"speculative\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        # Benchmark baseline decoding.\n",
        "        if compare_baseline:\n",
        "            for _ in range(num_runs):\n",
        "                inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "                input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "                attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "                with torch.no_grad():\n",
        "                    output_ids = self.target_model.generate(\n",
        "                        input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_length=input_ids.shape[1] + max_tokens,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=self.target_tokenizer.pad_token_id\n",
        "                    )\n",
        "                elapsed = time.time() - start_time\n",
        "                output_tokens = output_ids.shape[1] - input_ids.shape[1]\n",
        "                tps = output_tokens / elapsed\n",
        "                results[\"baseline\"][\"times\"].append(elapsed)\n",
        "                results[\"baseline\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        for method in results.keys():\n",
        "            if results[method] is not None:\n",
        "                avg_time = sum(results[method][\"times\"]) / num_runs\n",
        "                avg_tps = sum(results[method][\"tokens_per_second\"]) / num_runs\n",
        "                results[method][\"avg_time\"] = avg_time\n",
        "                results[method][\"avg_tokens_per_second\"] = avg_tps\n",
        "\n",
        "        if compare_baseline:\n",
        "            speedup = results[\"baseline\"][\"avg_time\"] / results[\"speculative\"][\"avg_time\"]\n",
        "            results[\"speedup\"] = speedup\n",
        "            results[\"latency_reduction\"] = (1 - results[\"speculative\"][\"avg_time\"] / results[\"baseline\"][\"avg_time\"]) * 100\n",
        "            # print(f\"Speculative decoding speedup: {speedup:.2f}x\")\n",
        "            # print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNzh3cG-6KM0"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YyNXbA-26Cpy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading draft model: EleutherAI/pythia-160m-deduped\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n",
            "Generated 106 tokens in 6.68 seconds\n",
            "Tokens per second: 15.88\n",
            "Draft token acceptance rate: 53.89%\n",
            "Generated 107 tokens in 3.78 seconds\n",
            "Tokens per second: 28.29\n",
            "Draft token acceptance rate: 43.11%\n",
            "Generated 113 tokens in 3.22 seconds\n",
            "Tokens per second: 35.08\n",
            "Draft token acceptance rate: 45.33%\n",
            "Average speculative decoding time: 4.58 seconds\n",
            "Average speculative tokens per second: 26.37\n",
            "Average baseline decoding time: 2.33 seconds\n",
            "Average baseline tokens per second: 43.67\n",
            "Speedup: 0.51x\n",
            "Latency reduction: -96.43%\n",
            "\n",
            "Benchmarking Prompt 2:\n",
            "Prompt: Write a short story about a robot learning to feel emotions:\n",
            "Generated 100 tokens in 4.00 seconds\n",
            "Tokens per second: 25.01\n",
            "Draft token acceptance rate: 31.93%\n",
            "Generated 106 tokens in 6.05 seconds\n",
            "Tokens per second: 17.51\n",
            "Draft token acceptance rate: 25.83%\n",
            "Generated 103 tokens in 8.75 seconds\n",
            "Tokens per second: 11.77\n",
            "Draft token acceptance rate: 21.48%\n",
            "Average speculative decoding time: 6.27 seconds\n",
            "Average speculative tokens per second: 18.09\n",
            "Average baseline decoding time: 3.03 seconds\n",
            "Average baseline tokens per second: 33.59\n",
            "Speedup: 0.48x\n",
            "Latency reduction: -106.86%\n",
            "\n",
            "Benchmarking Prompt 3:\n",
            "Prompt: Write the lyrics to the song 'Happy Birthday'.\n",
            "Generated 112 tokens in 2.77 seconds\n",
            "Tokens per second: 40.49\n",
            "Draft token acceptance rate: 57.22%\n",
            "Generated 110 tokens in 2.35 seconds\n",
            "Tokens per second: 46.78\n",
            "Draft token acceptance rate: 57.22%\n",
            "Generated 102 tokens in 2.36 seconds\n",
            "Tokens per second: 43.28\n",
            "Draft token acceptance rate: 56.36%\n",
            "Average speculative decoding time: 2.49 seconds\n",
            "Average speculative tokens per second: 43.45\n",
            "Average baseline decoding time: 1.88 seconds\n",
            "Average baseline tokens per second: 54.26\n",
            "Speedup: 0.75x\n",
            "Latency reduction: -32.96%\n"
          ]
        }
      ],
      "source": [
        "target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading draft model: EleutherAI/pythia-160m-deduped\n"
          ]
        }
      ],
      "source": [
        "# # test function\n",
        "# target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "# draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# # Initialize speculative decoder\n",
        "# decoder = SpeculativeDecoder(\n",
        "#     target_model_name=target_model_name,\n",
        "#     draft_model_name=draft_model_name,\n",
        "#     device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# )\n",
        "\n",
        "# # Test prompts\n",
        "# test_prompts = [\n",
        "#     \"The future of Artificial Intelligence is\",\n",
        "#     \"Write a short story about a robot learning to feel emotions:\",\n",
        "#     \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "# ]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated 103 tokens in 41.49 seconds\n",
            "Tokens per second: 2.48\n",
            "Draft token acceptance rate: 4.02%\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'The future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\nThe future of Artificial Intelligence is here.\\n\\n'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# decoder.speculative_decode(test_prompts[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "# def generate_draft_tokens(model, draft_tokenizer,input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
        "#                             num_speculative_tokens: int = 10) -> torch.Tensor:\n",
        "#     \"\"\"\n",
        "#     Generate speculative tokens in one forward call using the draft model.\n",
        "\n",
        "#     Args:\n",
        "#         input_ids: Input token IDs (tensor of shape [1, seq_len]).\n",
        "#         attention_mask: Corresponding attention mask.\n",
        "#         num_speculative_tokens: Number of tokens to speculate.\n",
        "\n",
        "#     Returns:\n",
        "#         Tensor of shape [1, num_speculative_tokens] containing the draft tokens.\n",
        "#     \"\"\"\n",
        "#     # TODO: Implement draft token generation\n",
        "#     # 1. Use the draft model to generate tokens\n",
        "#     # 2. Extract only the new tokens (not including the input)\n",
        "#     # 3. Return the newly generated tokens\n",
        "#     # baseline: directly generate tokens\n",
        "\n",
        "    \n",
        "#     max_tokens = num_speculative_tokens + input_ids.shape[-1]\n",
        "#     generation_config = GenerationConfig(\n",
        "#         max_new_tokens=num_speculative_tokens,\n",
        "#         do_sample=True,\n",
        "#         pad_token_id=draft_tokenizer.pad_token_id,\n",
        "#         eos_token_id=draft_tokenizer.eos_token_id,\n",
        "#         bos_token_id=draft_tokenizer.bos_token_id,\n",
        "#     )\n",
        "#     model.generation_config = generation_config\n",
        "#     draft = model.generate(\n",
        "#         input_ids=input_ids, \n",
        "#         attention_mask=attention_mask, \n",
        "#         # use_cache=True\n",
        "#     )\n",
        "\n",
        "#     return draft[:, input_ids.shape[-1]:]\n",
        "\n",
        "# inputs = decoder.target_tokenizer(test_prompts[0], return_tensors=\"pt\", padding=True)\n",
        "# input_ids = inputs[\"input_ids\"].to(decoder.device)\n",
        "# attention_mask = inputs[\"attention_mask\"].to(decoder.device)\n",
        "# prompt_length = input_ids.shape[1]\n",
        "\n",
        "# # Initialize counters for performance tracking\n",
        "# total_tokens_generated = prompt_length\n",
        "# total_draft_tokens_proposed = 0\n",
        "# total_draft_tokens_accepted = 0\n",
        "# start_time = time.time()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 3])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 3])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 6])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 6])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 1])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 1])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 6])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 2])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 5])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 8])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 9])\n",
            "torch.Size([1, 0])\n",
            "torch.Size([1, 1])\n"
          ]
        }
      ],
      "source": [
        "# total_tokens = 0\n",
        "# while total_tokens < 100 and input_ids[0, -1] != decoder.target_tokenizer.eos_token_id:\n",
        "#     draft_ids = generate_draft_tokens(decoder.draft_model, decoder.draft_tokenizer, input_ids, attention_mask, 10)\n",
        "\n",
        "#     # verification:\n",
        "#     target_input_ids = torch.cat([input_ids, draft_ids], dim=-1)\n",
        "#     target_attention_mask = torch.cat([attention_mask, torch.ones_like(draft_ids)], dim=-1)\n",
        "#     target_input_ids = target_input_ids.to(decoder.device)\n",
        "#     target_attention_mask = target_attention_mask.to(decoder.device)\n",
        "#     with torch.no_grad():\n",
        "#         logits = decoder.target_model(target_input_ids, attention_mask=target_attention_mask).logits\n",
        "        \n",
        "#     start_idx = input_ids.shape[-1]-1\n",
        "#     target_draft_logits = logits[:, start_idx:-1, :]\n",
        "\n",
        "#     # get target tokens\n",
        "#     target_predicted_tokens = target_draft_logits.argmax(dim=-1)\n",
        "\n",
        "#     # matches tokens\n",
        "#     match_mask = ~(target_predicted_tokens == draft_ids).int()\n",
        "#     if match_mask.sum() != 0:\n",
        "#         accept_position = torch.argmax(match_mask, dim=-1)\n",
        "#     else:\n",
        "#         accept_position = draft_ids.shape[-1]\n",
        "\n",
        "#     accepted_tokens = draft_ids[:,:accept_position]\n",
        "\n",
        "#     # append accepted tokens to the sequence\n",
        "#     print(accepted_tokens.shape)\n",
        "\n",
        "#     input_ids = torch.cat([input_ids, accepted_tokens], dim=-1)\n",
        "#     attention_mask = torch.cat([attention_mask, torch.ones_like(accepted_tokens)], dim=-1)\n",
        "#     total_tokens = input_ids.shape[-1]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O1EORd26MdC"
      },
      "source": [
        "## Bonus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1sEo2706O29"
      },
      "outputs": [],
      "source": [
        "target_model_name = ...  # Larger target model\n",
        "draft_model_name = ...   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

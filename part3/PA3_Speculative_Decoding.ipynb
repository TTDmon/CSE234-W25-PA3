{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmRSlH1L5r-F"
      },
      "source": [
        "# CSE 234 Programming Assignment 3: Speculative Decoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zDuj8yGG6EXg"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "52T8Gw-R5lup"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/xunyijiang/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
        "from typing import List, Tuple, Dict, Optional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwyZ4tAb6Gu2"
      },
      "source": [
        "## Speculative Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "VIvmDG725x8H"
      },
      "outputs": [],
      "source": [
        "class SpeculativeDecoder:\n",
        "    def __init__(self, target_model_name: str, draft_model_name: str, device: str = \"cuda\"):\n",
        "        \"\"\"\n",
        "        Initialize the speculative decoder with target and draft models.\n",
        "\n",
        "        Args:\n",
        "            target_model_name: HuggingFace model ID for the larger target model.\n",
        "            draft_model_name: HuggingFace model ID for the smaller draft model.\n",
        "            device: Device to run models on (\"cuda\" or \"cpu\").\n",
        "        \"\"\"\n",
        "        self.device = device\n",
        "        self.target_model, self.target_tokenizer = self.initialize_target_model(target_model_name)\n",
        "        self.draft_model, self.draft_tokenizer = self.initialize_draft_model(draft_model_name)\n",
        "\n",
        "        # Ensure tokenizers are compatible\n",
        "        assert self.target_tokenizer.vocab == self.draft_tokenizer.vocab, \"Tokenizers must be compatible\"\n",
        "\n",
        "    def initialize_target_model(self, model_name: str):\n",
        "        \"\"\"Initialize the larger target model with caching enabled and proper pad token.\"\"\"\n",
        "        print(f\"Loading target model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement target model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        # baseline: directly load the model\n",
        "        # pad token:\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        model.to(self.device)\n",
        "        return model, tokenizer\n",
        "\n",
        "    def initialize_draft_model(self, model_name: str):\n",
        "        \"\"\"\n",
        "        Initialize a smaller, faster draft model with proper pad token.\n",
        "        Uses lower precision and additional optimizations.\n",
        "        \"\"\"\n",
        "        print(f\"Loading draft model: {model_name}\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # TODO: Implement draft model initialization\n",
        "        # 1. Set the pad token if it doesn't exist\n",
        "        # 2. Load the model with appropriate settings for inference\n",
        "        # 3. Enable any optimizations that might help with performance\n",
        "        if not hasattr(self, 'target_model'):\n",
        "            self.target_model, self.target_tokenizer = self.initialize_target_model(self.target_model_name)\n",
        "\n",
        "        # pad token:\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "        # baseline: directly load the model \n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "        model.to(self.device)\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    def generate_draft_tokens(self, input_ids: torch.Tensor, attention_mask: torch.Tensor,\n",
        "                             num_speculative_tokens: int = 10) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Generate speculative tokens in one forward call using the draft model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: Input token IDs (tensor of shape [1, seq_len]).\n",
        "            attention_mask: Corresponding attention mask.\n",
        "            num_speculative_tokens: Number of tokens to speculate.\n",
        "\n",
        "        Returns:\n",
        "            Tensor of shape [1, num_speculative_tokens] containing the draft tokens.\n",
        "        \"\"\"\n",
        "        # TODO: Implement draft token generation\n",
        "        # 1. Use the draft model to generate tokens\n",
        "        # 2. Extract only the new tokens (not including the input)\n",
        "        # 3. Return the newly generated tokens\n",
        "        # baseline: directly generate tokens\n",
        "        if not hasattr(self, 'draft_model'):\n",
        "            self.draft_model, self.draft_tokenizer = self.initialize_draft_model(self.draft_model_name)\n",
        "        \n",
        "        max_tokens = num_speculative_tokens + input_ids.shape[-1]\n",
        "        generation_config = GenerationConfig(\n",
        "            max_new_tokens=num_speculative_tokens,\n",
        "            do_sample=True,\n",
        "            # temperature=0.3,\n",
        "            pad_token_id=self.draft_tokenizer.pad_token_id,\n",
        "            eos_token_id=self.draft_tokenizer.eos_token_id,\n",
        "            bos_token_id=self.draft_tokenizer.bos_token_id,\n",
        "        )\n",
        "        self.draft_model.generation_config = generation_config\n",
        "        draft = self.draft_model.generate(\n",
        "            input_ids=input_ids, \n",
        "            attention_mask=attention_mask, \n",
        "            use_cache=True,\n",
        "            temperature=0.34,  # 降低 temperature，生成更确定的 token\n",
        "            top_k=50,         # 限制 token 选择范围\n",
        "            top_p=0.85,        # 降低多样性，避免 draft model 生成错误 token\n",
        "        )\n",
        "\n",
        "        return draft[:, input_ids.shape[-1]:]\n",
        "\n",
        "    def verify_tokens_vectorized(self, input_ids: torch.Tensor, draft_tokens: torch.Tensor,\n",
        "                               attention_mask: torch.Tensor) -> Tuple[List[int], int]:\n",
        "        \"\"\"\n",
        "        Vectorized verification: verify all draft tokens in one forward pass using the target model.\n",
        "\n",
        "        Args:\n",
        "            input_ids: The current input token IDs (shape [1, L]).\n",
        "            draft_tokens: Draft tokens from the draft model (shape [1, k]).\n",
        "            attention_mask: The current attention mask for input_ids.\n",
        "\n",
        "        Returns:\n",
        "            accepted_tokens: List of accepted token IDs.\n",
        "            accepted_position: Index of the first rejected token (if all accepted, equals draft_tokens.shape[1]).\n",
        "        \"\"\"\n",
        "        # TODO: Implement efficient verification of draft tokens\n",
        "        # 1. Run target model on input_ids concatenated with draft_tokens\n",
        "        # 2. Extract the logits for positions where draft tokens would be predicted\n",
        "        # 3. Compare target model predictions with draft tokens\n",
        "        # 4. Determine how many consecutive tokens were accepted before first mismatch\n",
        "        target_input_ids = torch.cat([input_ids, draft_tokens], dim=-1)\n",
        "        target_attention_mask = torch.cat([attention_mask, torch.ones_like(draft_tokens)], dim=-1)\n",
        "        target_input_ids = target_input_ids.to(self.device)\n",
        "        target_attention_mask = target_attention_mask.to(self.device)\n",
        "        with torch.no_grad():\n",
        "            logits = self.target_model(target_input_ids, attention_mask=target_attention_mask).logits\n",
        "        \n",
        "        draft_start_idx = input_ids.shape[-1]-1\n",
        "        target_draft_logits = logits[:, draft_start_idx:-1, :]\n",
        "\n",
        "        # get target tokens\n",
        "        target_predicted_tokens = target_draft_logits.argmax(dim=-1)\n",
        "\n",
        "        # matches tokens\n",
        "        match_mask = (target_predicted_tokens != draft_tokens).int() # 0 if match, 1 if mismatch\n",
        "\n",
        "        if match_mask.sum() != 0:\n",
        "            accept_position = torch.argmax(match_mask, dim=-1)\n",
        "        else:\n",
        "            accept_position = draft_tokens.shape[-1]\n",
        "\n",
        "        accepted_tokens = draft_tokens[:,:accept_position]\n",
        "        return accepted_tokens, accept_position\n",
        "    \n",
        "    def verify_tokens_cached(self, input_ids: torch.Tensor, draft_tokens: torch.Tensor,\n",
        "                             attention_mask: torch.Tensor, past_key_values=None):\n",
        "        \"\"\"\n",
        "        Vectorized verification: verify all draft tokens in one forward pass using the target model.\n",
        "        \"\"\"\n",
        "        input_ids = input_ids.long()\n",
        "        attention_mask = attention_mask.long()\n",
        "        draft_tokens = draft_tokens.long()\n",
        "\n",
        "        target_input_ids = torch.cat([input_ids, draft_tokens], dim=-1)\n",
        "        target_attention_mask = torch.cat([attention_mask, torch.ones_like(draft_tokens)], dim=-1)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.target_model(\n",
        "                target_input_ids,\n",
        "                attention_mask=target_attention_mask,\n",
        "                # past_key_values=past_key_values,  # 传入历史 KV Cache\n",
        "                use_cache=True  # 让模型存储 KV Cache\n",
        "            )\n",
        "        \n",
        "        logits = outputs.logits\n",
        "\n",
        "        draft_start_idx = input_ids.shape[-1]-1\n",
        "        target_draft_logits = logits[:, draft_start_idx:-1, :]\n",
        "        target_predicted_tokens = target_draft_logits.argmax(dim=-1)\n",
        "\n",
        "        match_mask = (target_predicted_tokens != draft_tokens).int()  # 1 if mismatch, 0 if match\n",
        "\n",
        "        if match_mask.sum() > 0:\n",
        "            accept_position = torch.argmax(match_mask, dim=-1)\n",
        "        else:\n",
        "            accept_position = draft_tokens.shape[-1]\n",
        "\n",
        "        accepted_tokens = draft_tokens[:, :accept_position]\n",
        "        # new_past_key_values = outputs.past_key_values  \n",
        "        new_past_key_values = tuple(\n",
        "            (layer_K[:, :, :input_ids.shape[-1] + accept_position, :], layer_V[:, :, :input_ids.shape[-1] + accept_position, :])\n",
        "            for layer_K, layer_V in outputs.past_key_values\n",
        "        )\n",
        "\n",
        "        return accepted_tokens, accept_position, new_past_key_values  \n",
        "\n",
        "    def speculative_decode(self, prompt: str, max_tokens: int = 100,\n",
        "                          num_speculative_tokens: int = 15, use_cached: bool = False) -> str:\n",
        "        \"\"\"\n",
        "        Main speculative decoding algorithm with vectorized verification.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate (excluding prompt).\n",
        "            num_speculative_tokens: Number of tokens to speculate per iteration.\n",
        "\n",
        "        Returns:\n",
        "            Generated text.\n",
        "        \"\"\"\n",
        "        # Tokenize prompt\n",
        "        inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "        input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "        attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "        prompt_length = input_ids.shape[1]\n",
        "\n",
        "        # Initialize counters for performance tracking\n",
        "        total_tokens_generated = prompt_length\n",
        "        total_draft_tokens_proposed = 0\n",
        "        total_draft_tokens_accepted = 0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # TODO: Implement the core speculative decoding loop\n",
        "        # 1. Generate draft tokens using the draft model\n",
        "        # 2. Verify draft tokens using the target model\n",
        "        # 3. Accept verified tokens and append to the sequence\n",
        "        # 4. For rejected tokens or if all tokens are accepted, generate a new token with the target model\n",
        "        # 5. Stop when max_tokens is reached or an EOS token is generated\n",
        "        past_key_values = None\n",
        "        \n",
        "        while total_tokens_generated - prompt_length < max_tokens and input_ids[0, -1] != self.target_tokenizer.eos_token_id:   \n",
        "            # draft generation:\n",
        "            draft_ids = self.generate_draft_tokens(input_ids, attention_mask, num_speculative_tokens)\n",
        "            total_draft_tokens_proposed += num_speculative_tokens\n",
        "\n",
        "            # verification:\n",
        "            if use_cached:\n",
        "                accepted_draft_ids, accepted_position, new_past_key_values = self.verify_tokens_cached(input_ids, draft_ids, attention_mask, past_key_values)\n",
        "                past_key_values = new_past_key_values\n",
        "                total_draft_tokens_accepted += accepted_draft_ids.shape[-1]\n",
        "\n",
        "                input_ids = torch.cat([input_ids, accepted_draft_ids], dim=-1)\n",
        "                attention_mask = torch.cat([attention_mask, torch.ones_like(accepted_draft_ids)], dim=-1)\n",
        "\n",
        "                if accepted_position == num_speculative_tokens or accepted_position == 0:\n",
        "                    input_ids = self.target_model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_new_tokens=1,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=self.target_tokenizer.pad_token_id,\n",
        "                        use_cache=True,\n",
        "                        # past_key_values=past_key_values\n",
        "                    )\n",
        "\n",
        "\n",
        "                    # input_ids = torch.cat([input_ids, new_token], dim=-1)\n",
        "                    attention_mask = torch.cat([attention_mask, torch.ones_like(torch.tensor([[1]]).to(self.device))], dim=-1)\n",
        "\n",
        "            else:\n",
        "                accepted_draft_ids, accepted_position = self.verify_tokens_vectorized(input_ids, draft_ids, attention_mask)\n",
        "                total_draft_tokens_accepted += accepted_draft_ids.shape[-1]\n",
        "                # print(\"accepted_draft_ids\", accepted_draft_ids)\n",
        "\n",
        "                # print(\"before concatenating accepted_draft_ids:\", input_ids.shape[-1])\n",
        "\n",
        "                # append accepted tokens to the sequence\n",
        "                input_ids = torch.cat([input_ids, accepted_draft_ids], dim=-1)\n",
        "                attention_mask = torch.cat([attention_mask, torch.ones_like(accepted_draft_ids)], dim=-1)\n",
        "                \n",
        "                # print(\"Before concatenating new token:\", input_ids.shape[-1])\n",
        "                # if all accepted or rejected, generate a new token with the target model\n",
        "                if accepted_position == num_speculative_tokens or accepted_position == 0:\n",
        "                    # print(\"Start generating a new token!!!!\")\n",
        "                    # all accepted, generate a new token with the target model\n",
        "                    input_ids = self.target_model.generate(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_new_tokens=1,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=self.target_tokenizer.pad_token_id\n",
        "                    )\n",
        "                    # print(\"new_token\", input_ids)\n",
        "                    attention_mask = torch.cat([attention_mask, torch.tensor([[1]]).to(self.device)], dim=-1)\n",
        "\n",
        "                # update counters\n",
        "            total_tokens_generated = input_ids.shape[-1]\n",
        "\n",
        "        # Calculate performance metrics\n",
        "        elapsed_time = time.time() - start_time\n",
        "        acceptance_rate = total_draft_tokens_accepted / total_draft_tokens_proposed if total_draft_tokens_proposed > 0 else 0\n",
        "\n",
        "        print(f\"Generated {total_tokens_generated - prompt_length} tokens in {elapsed_time:.2f} seconds\")\n",
        "        print(f\"Tokens per second: {(total_tokens_generated - prompt_length) / elapsed_time:.2f}\")\n",
        "        print(f\"Draft token acceptance rate: {acceptance_rate:.2%}\")\n",
        "\n",
        "        return self.target_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
        "\n",
        "    def benchmark(self, prompt: str, max_tokens: int = 100,\n",
        "                  num_runs: int = 3, compare_baseline: bool = True, use_cached: bool = False, num_speculative_tokens: int = 15) -> Dict:\n",
        "        \"\"\"\n",
        "        Benchmark the speculative decoder against baseline decoding.\n",
        "\n",
        "        Args:\n",
        "            prompt: Input text.\n",
        "            max_tokens: Maximum number of tokens to generate.\n",
        "            num_runs: Number of benchmark runs.\n",
        "            compare_baseline: Whether to compare with baseline (non-speculative) decoding.\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with benchmark results.\n",
        "        \"\"\"\n",
        "        results = {\n",
        "            \"speculative\": {\"times\": [], \"tokens_per_second\": []},\n",
        "            \"baseline\": {\"times\": [], \"tokens_per_second\": []} if compare_baseline else None\n",
        "        }\n",
        "\n",
        "        # Benchmark speculative decoding.\n",
        "        for _ in range(num_runs):\n",
        "            start_time = time.time()\n",
        "            output = self.speculative_decode(prompt, max_tokens=max_tokens, use_cached=use_cached, num_speculative_tokens=num_speculative_tokens)\n",
        "            elapsed = time.time() - start_time\n",
        "            prompt_len = len(self.target_tokenizer(prompt)[\"input_ids\"])\n",
        "            output_tokens = len(self.target_tokenizer.encode(output)) - prompt_len\n",
        "            tps = output_tokens / elapsed\n",
        "            results[\"speculative\"][\"times\"].append(elapsed)\n",
        "            results[\"speculative\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        # Benchmark baseline decoding.\n",
        "        if compare_baseline:\n",
        "            for _ in range(num_runs):\n",
        "                inputs = self.target_tokenizer(prompt, return_tensors=\"pt\", padding=True)\n",
        "                input_ids = inputs[\"input_ids\"].to(self.device)\n",
        "                attention_mask = inputs[\"attention_mask\"].to(self.device)\n",
        "                start_time = time.time()\n",
        "                with torch.no_grad():\n",
        "                    output_ids = self.target_model.generate(\n",
        "                        input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        max_length=input_ids.shape[1] + max_tokens,\n",
        "                        do_sample=False,\n",
        "                        pad_token_id=self.target_tokenizer.pad_token_id\n",
        "                    )\n",
        "                elapsed = time.time() - start_time\n",
        "                output_tokens = output_ids.shape[1] - input_ids.shape[1]\n",
        "                tps = output_tokens / elapsed\n",
        "                results[\"baseline\"][\"times\"].append(elapsed)\n",
        "                results[\"baseline\"][\"tokens_per_second\"].append(tps)\n",
        "\n",
        "        for method in results.keys():\n",
        "            if results[method] is not None:\n",
        "                avg_time = sum(results[method][\"times\"]) / num_runs\n",
        "                avg_tps = sum(results[method][\"tokens_per_second\"]) / num_runs\n",
        "                results[method][\"avg_time\"] = avg_time\n",
        "                results[method][\"avg_tokens_per_second\"] = avg_tps\n",
        "\n",
        "        if compare_baseline:\n",
        "            speedup = results[\"baseline\"][\"avg_time\"] / results[\"speculative\"][\"avg_time\"]\n",
        "            results[\"speedup\"] = speedup\n",
        "            results[\"latency_reduction\"] = (1 - results[\"speculative\"][\"avg_time\"] / results[\"baseline\"][\"avg_time\"]) * 100\n",
        "            # print(f\"Speculative decoding speedup: {speedup:.2f}x\")\n",
        "            # print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")\n",
        "\n",
        "        return results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNzh3cG-6KM0"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "YyNXbA-26Cpy"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading draft model: EleutherAI/pythia-160m-deduped\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n",
            "Generated 109 tokens in 6.01 seconds\n",
            "Tokens per second: 18.15\n",
            "Draft token acceptance rate: 46.67%\n",
            "Generated 107 tokens in 5.11 seconds\n",
            "Tokens per second: 20.93\n",
            "Draft token acceptance rate: 46.19%\n",
            "Generated 103 tokens in 4.13 seconds\n",
            "Tokens per second: 24.92\n",
            "Draft token acceptance rate: 56.97%\n",
            "Average speculative decoding time: 5.09 seconds\n",
            "Average speculative tokens per second: 21.22\n",
            "Average baseline decoding time: 2.64 seconds\n",
            "Average baseline tokens per second: 37.91\n",
            "Speedup: 0.52x\n",
            "Latency reduction: -93.06%\n",
            "\n",
            "Benchmarking Prompt 2:\n",
            "Prompt: Write a short story about a robot learning to feel emotions:\n",
            "Generated 104 tokens in 9.41 seconds\n",
            "Tokens per second: 11.06\n",
            "Draft token acceptance rate: 22.82%\n",
            "Generated 115 tokens in 7.13 seconds\n",
            "Tokens per second: 16.12\n",
            "Draft token acceptance rate: 35.00%\n",
            "Generated 106 tokens in 6.65 seconds\n",
            "Tokens per second: 15.94\n",
            "Draft token acceptance rate: 34.74%\n",
            "Average speculative decoding time: 7.74 seconds\n",
            "Average speculative tokens per second: 14.36\n",
            "Average baseline decoding time: 2.64 seconds\n",
            "Average baseline tokens per second: 37.83\n",
            "Speedup: 0.34x\n",
            "Latency reduction: -192.66%\n",
            "\n",
            "Benchmarking Prompt 3:\n",
            "Prompt: Write the lyrics to the song 'Happy Birthday'.\n",
            "Generated 103 tokens in 5.33 seconds\n",
            "Tokens per second: 19.32\n",
            "Draft token acceptance rate: 42.67%\n",
            "Generated 100 tokens in 4.23 seconds\n",
            "Tokens per second: 23.66\n",
            "Draft token acceptance rate: 52.22%\n",
            "Generated 113 tokens in 4.64 seconds\n",
            "Tokens per second: 24.37\n",
            "Draft token acceptance rate: 54.87%\n",
            "Average speculative decoding time: 4.74 seconds\n",
            "Average speculative tokens per second: 22.42\n",
            "Average baseline decoding time: 2.64 seconds\n",
            "Average baseline tokens per second: 37.93\n",
            "Speedup: 0.56x\n",
            "Latency reduction: -79.71%\n"
          ]
        }
      ],
      "source": [
        "target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use cached KV Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n",
            "Loading draft model: EleutherAI/pythia-160m-deduped\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n",
            "Generated 109 tokens in 1.42 seconds\n",
            "Tokens per second: 76.87\n",
            "Draft token acceptance rate: 83.33%\n",
            "Generated 105 tokens in 1.30 seconds\n",
            "Tokens per second: 80.96\n",
            "Draft token acceptance rate: 88.89%\n",
            "Generated 105 tokens in 1.30 seconds\n",
            "Tokens per second: 81.03\n",
            "Draft token acceptance rate: 88.89%\n",
            "Average speculative decoding time: 1.34 seconds\n",
            "Average speculative tokens per second: 79.57\n",
            "Average baseline decoding time: 1.48 seconds\n",
            "Average baseline tokens per second: 67.42\n",
            "Speedup: 1.11x\n",
            "Latency reduction: 9.80%\n",
            "\n",
            "Benchmarking Prompt 2:\n",
            "Prompt: Write a short story about a robot learning to feel emotions:\n",
            "Generated 112 tokens in 1.32 seconds\n",
            "Tokens per second: 84.78\n",
            "Draft token acceptance rate: 96.30%\n",
            "Generated 112 tokens in 1.67 seconds\n",
            "Tokens per second: 67.04\n",
            "Draft token acceptance rate: 71.53%\n",
            "Generated 110 tokens in 1.29 seconds\n",
            "Tokens per second: 85.11\n",
            "Draft token acceptance rate: 94.44%\n",
            "Average speculative decoding time: 1.43 seconds\n",
            "Average speculative tokens per second: 78.91\n",
            "Average baseline decoding time: 1.49 seconds\n",
            "Average baseline tokens per second: 67.14\n",
            "Speedup: 1.04x\n",
            "Latency reduction: 4.04%\n",
            "\n",
            "Benchmarking Prompt 3:\n",
            "Prompt: Write the lyrics to the song 'Happy Birthday'.\n",
            "Generated 107 tokens in 1.43 seconds\n",
            "Tokens per second: 74.72\n",
            "Draft token acceptance rate: 81.67%\n",
            "Generated 107 tokens in 1.43 seconds\n",
            "Tokens per second: 74.95\n",
            "Draft token acceptance rate: 81.67%\n",
            "Generated 107 tokens in 1.43 seconds\n",
            "Tokens per second: 74.61\n",
            "Draft token acceptance rate: 81.67%\n",
            "Average speculative decoding time: 1.43 seconds\n",
            "Average speculative tokens per second: 74.69\n",
            "Average baseline decoding time: 1.48 seconds\n",
            "Average baseline tokens per second: 67.38\n",
            "Speedup: 1.04x\n",
            "Latency reduction: 3.47%\n"
          ]
        }
      ],
      "source": [
        "target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True,\n",
        "        use_cached=True,\n",
        "        num_speculative_tokens=12\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1O1EORd26MdC"
      },
      "source": [
        "## Bonus1\n",
        "Different model pairs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Y1sEo2706O29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-6.9b\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading shards: 100%|██████████| 2/2 [02:00<00:00, 60.33s/it]\n",
            "The `GPTNeoXSdpaAttention` class is deprecated in favor of simply modifying the `config._attn_implementation`attribute of the `GPTNeoXAttention` class! It will be removed in v4.48\n",
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.40s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading draft model: EleutherAI/pythia-1.4b\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n",
            "Generated 112 tokens in 3.74 seconds\n",
            "Tokens per second: 29.91\n",
            "Draft token acceptance rate: 100.00%\n",
            "Generated 112 tokens in 2.91 seconds\n",
            "Tokens per second: 38.54\n",
            "Draft token acceptance rate: 100.00%\n",
            "Generated 114 tokens in 3.17 seconds\n",
            "Tokens per second: 35.91\n",
            "Draft token acceptance rate: 89.17%\n",
            "Average speculative decoding time: 3.28 seconds\n",
            "Average speculative tokens per second: 34.64\n",
            "Average baseline decoding time: 4.14 seconds\n",
            "Average baseline tokens per second: 24.18\n",
            "Speedup: 1.26x\n",
            "Latency reduction: 20.70%\n",
            "\n",
            "Benchmarking Prompt 2:\n",
            "Prompt: Write a short story about a robot learning to feel emotions:\n",
            "Generated 112 tokens in 10.25 seconds\n",
            "Tokens per second: 10.93\n",
            "Draft token acceptance rate: 22.53%\n",
            "Generated 112 tokens in 9.51 seconds\n",
            "Tokens per second: 11.78\n",
            "Draft token acceptance rate: 24.69%\n",
            "Generated 112 tokens in 7.61 seconds\n",
            "Tokens per second: 14.72\n",
            "Draft token acceptance rate: 31.75%\n",
            "Average speculative decoding time: 9.12 seconds\n",
            "Average speculative tokens per second: 12.47\n",
            "Average baseline decoding time: 4.14 seconds\n",
            "Average baseline tokens per second: 24.16\n",
            "Speedup: 0.45x\n",
            "Latency reduction: -120.38%\n",
            "\n",
            "Benchmarking Prompt 3:\n",
            "Prompt: Write the lyrics to the song 'Happy Birthday'.\n",
            "Generated 114 tokens in 3.25 seconds\n",
            "Tokens per second: 35.12\n",
            "Draft token acceptance rate: 89.17%\n",
            "Generated 115 tokens in 3.57 seconds\n",
            "Tokens per second: 32.25\n",
            "Draft token acceptance rate: 79.26%\n",
            "Generated 115 tokens in 3.56 seconds\n",
            "Tokens per second: 32.26\n",
            "Draft token acceptance rate: 79.26%\n",
            "Average speculative decoding time: 3.46 seconds\n",
            "Average speculative tokens per second: 33.20\n",
            "Average baseline decoding time: 4.14 seconds\n",
            "Average baseline tokens per second: 24.15\n",
            "Speedup: 1.20x\n",
            "Latency reduction: 16.44%\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "# Define model pair (choose the best combination)\n",
        "target_model_name = \"EleutherAI/pythia-6.9b\"\n",
        "draft_model_name = \"EleutherAI/pythia-1.4b\"\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bonus\n",
        "Higher acceptance rate:\n",
        "When we want higher acceptance rate, we can generate less speculative tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading target model: EleutherAI/pythia-1.4b-deduped\n",
            "Loading draft model: EleutherAI/pythia-160m-deduped\n",
            "\n",
            "Benchmarking Prompt 1:\n",
            "Prompt: The future of Artificial Intelligence is\n",
            "Generated 103 tokens in 1.52 seconds\n",
            "Tokens per second: 67.66\n",
            "Draft token acceptance rate: 94.44%\n",
            "Generated 103 tokens in 1.51 seconds\n",
            "Tokens per second: 68.27\n",
            "Draft token acceptance rate: 94.44%\n",
            "Generated 100 tokens in 1.49 seconds\n",
            "Tokens per second: 67.31\n",
            "Draft token acceptance rate: 92.22%\n",
            "Average speculative decoding time: 1.51 seconds\n",
            "Average speculative tokens per second: 67.25\n",
            "Average baseline decoding time: 1.50 seconds\n",
            "Average baseline tokens per second: 66.83\n",
            "Speedup: 0.99x\n",
            "Latency reduction: -0.69%\n",
            "\n",
            "Benchmarking Prompt 2:\n",
            "Prompt: Write a short story about a robot learning to feel emotions:\n",
            "Generated 105 tokens in 1.67 seconds\n",
            "Tokens per second: 62.92\n",
            "Draft token acceptance rate: 86.00%\n",
            "Generated 104 tokens in 1.59 seconds\n",
            "Tokens per second: 65.36\n",
            "Draft token acceptance rate: 90.53%\n",
            "Generated 105 tokens in 1.59 seconds\n",
            "Tokens per second: 66.11\n",
            "Draft token acceptance rate: 91.58%\n",
            "Average speculative decoding time: 1.62 seconds\n",
            "Average speculative tokens per second: 64.75\n",
            "Average baseline decoding time: 1.49 seconds\n",
            "Average baseline tokens per second: 66.92\n",
            "Speedup: 0.92x\n",
            "Latency reduction: -8.23%\n",
            "\n",
            "Benchmarking Prompt 3:\n",
            "Prompt: Write the lyrics to the song 'Happy Birthday'.\n",
            "Generated 105 tokens in 1.58 seconds\n",
            "Tokens per second: 66.34\n",
            "Draft token acceptance rate: 91.58%\n",
            "Generated 105 tokens in 1.59 seconds\n",
            "Tokens per second: 66.18\n",
            "Draft token acceptance rate: 91.58%\n",
            "Generated 105 tokens in 1.58 seconds\n",
            "Tokens per second: 66.28\n",
            "Draft token acceptance rate: 91.58%\n",
            "Average speculative decoding time: 1.59 seconds\n",
            "Average speculative tokens per second: 66.21\n",
            "Average baseline decoding time: 1.50 seconds\n",
            "Average baseline tokens per second: 66.80\n",
            "Speedup: 0.94x\n",
            "Latency reduction: -5.93%\n"
          ]
        }
      ],
      "source": [
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
        "# Define model pair (choose the best combination)\n",
        "target_model_name = \"EleutherAI/pythia-1.4b-deduped\"  # Larger target model\n",
        "draft_model_name = \"EleutherAI/pythia-160m-deduped\"   # Smaller draft model\n",
        "\n",
        "\n",
        "\n",
        "# Initialize speculative decoder\n",
        "decoder = SpeculativeDecoder(\n",
        "    target_model_name=target_model_name,\n",
        "    draft_model_name=draft_model_name,\n",
        "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        ")\n",
        "\n",
        "# Test prompts\n",
        "test_prompts = [\n",
        "    \"The future of Artificial Intelligence is\",\n",
        "    \"Write a short story about a robot learning to feel emotions:\",\n",
        "    \"Write the lyrics to the song 'Happy Birthday'.\"\n",
        "]\n",
        "\n",
        "# Run benchmark on test prompts\n",
        "for i, prompt in enumerate(test_prompts):\n",
        "    print(f\"\\nBenchmarking Prompt {i+1}:\")\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    results = decoder.benchmark(\n",
        "        prompt=prompt,\n",
        "        max_tokens=100,\n",
        "        num_runs=3,\n",
        "        compare_baseline=True,\n",
        "        num_speculative_tokens=5,\n",
        "        use_cached=True\n",
        "    )\n",
        "\n",
        "    print(f\"Average speculative decoding time: {results['speculative']['avg_time']:.2f} seconds\")\n",
        "    print(f\"Average speculative tokens per second: {results['speculative']['avg_tokens_per_second']:.2f}\")\n",
        "\n",
        "    if results[\"baseline\"] is not None:\n",
        "        print(f\"Average baseline decoding time: {results['baseline']['avg_time']:.2f} seconds\")\n",
        "        print(f\"Average baseline tokens per second: {results['baseline']['avg_tokens_per_second']:.2f}\")\n",
        "        print(f\"Speedup: {results['speedup']:.2f}x\")\n",
        "        print(f\"Latency reduction: {results['latency_reduction']:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
